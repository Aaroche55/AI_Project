animate(p, duration = 5, fps = 10, width = 600, height = 400)
install.packages("hexbin")
library(hexbin)
ggplot(df, aes(x = lexical_diversity, y = entropy)) +
geom_hex(bins = 30) +
scale_fill_viridis_c() +  # Uses a beautiful color scale
labs(title = "Hexbin Density Plot: Lexical Diversity vs. Entropy", x = "Lexical Diversity", y = "Entropy") +
theme_minimal()
install.packages("GGally")
library(GGally)
ggparcoord(df, columns = 1:6, groupColumn = "generated", alpha = 0.5) +
labs(title = "Parallel Coordinates Plot: AI vs. Human Features") +
theme_minimal()
install.packages("umap")
library(umap)
library(ggplot2)
# Run UMAP on numerical data
umap_results <- umap(df[, 1:6])  # First 6 features
install.packages("wordcloud")
library(wordcloud)
# Sample fake word frequencies (replace with real data)
word_freq <- c("data" = 100, "learning" = 80, "text" = 90, "AI" = 120, "human" = 70)
# Generate word cloud
wordcloud(words = names(word_freq), freq = word_freq, min.freq = 2,
colors = brewer.pal(8, "Dark2"), random.order = FALSE)
install.packages("wordcloud")
install.packages("dplyr")
library(wordcloud)
library(dplyr)
# Create a dummy word frequency based on `ai_words_rate`
# For simplicity, we'll assume higher `ai_words_rate` means more AI-related words
# Create a vector of words and their corresponding frequencies based on ai_words_rate
ai_words_data <- df %>%
mutate(word = paste("AI_Word_", 1:nrow(df), sep = ""),
frequency = round(ai_words_rate * 1000)) %>%  # Scale the frequency for visibility
select(word, frequency)
# Create the word cloud
wordcloud(words = ai_words_data$word, freq = ai_words_data$frequency,
min.freq = 1, scale = c(3,0.5), colors = brewer.pal(8, "Dark2"))
install.packages("wordcloud")
install.packages("dplyr")
library(wordcloud)
library(dplyr)
# Create a dummy word frequency based on `ai_words_rate`
# For simplicity, we'll assume higher `ai_words_rate` means more AI-related words
# Create a vector of words and their corresponding frequencies based on ai_words_rate
ai_words_data <- df %>%
mutate(word = paste("AI_Word_", 1:nrow(df), sep = ""),
frequency = round(ai_words_rate * 1000)) %>%  # Scale the frequency for visibility
select(word, frequency)
# Create the word cloud
wordcloud(words = ai_words_data$word, freq = ai_words_data$frequency,
min.freq = 1, scale = c(3,0.5), colors = brewer.pal(8, "Dark2"))
library(wordcloud)
library(dplyr)
# Create a dummy word frequency based on `ai_words_rate`
# For simplicity, we'll assume higher `ai_words_rate` means more AI-related words
# Create a vector of words and their corresponding frequencies based on ai_words_rate
ai_words_data <- df %>%
mutate(word = paste("AI_Word_", 1:nrow(df), sep = ""),
frequency = round(ai_words_rate * 1000)) %>%  # Scale the frequency for visibility
select(word, frequency)
# Create the word cloud
wordcloud(words = ai_words_data$word, freq = ai_words_data$frequency,
min.freq = 1, scale = c(3,0.5), colors = brewer.pal(8, "Dark2"))
install.packages("gganimate")
library(gganimate)
library(ggplot2)
# Create the scatter plot with animation
p <- ggplot(df, aes(x = zipf_adherence, y = ai_words_rate, color = as.factor(generated))) +
geom_point(alpha = 0.7, size = 3) +
labs(title = 'AI vs. Human Text Analysis: Frame {frame}', x = 'Zipf Adherence', y = 'AI Words Rate') +
theme_minimal() +
transition_states(generated, transition_length = 2, state_length = 1) +
ease_aes('linear')
# Animate the plot and save it
anim <- animate(p, duration = 5, fps = 10, width = 600, height = 400)
library(gganimate)
library(ggplot2)
# Create the scatter plot with animation
p <- ggplot(df, aes(x = zipf_adherence, y = ai_words_rate, color = as.factor(generated))) +
geom_point(alpha = 0.7, size = 3) +
labs(title = 'AI vs. Human Text Analysis: Frame {frame}', x = 'Zipf Adherence', y = 'AI Words Rate') +
theme_minimal() +
transition_states(generated, transition_length = 2, state_length = 1) +
ease_aes('linear')
# Animate the plot and save it
anim <- animate(p, duration = 5, fps = 10, width = 600, height = 400)
# Save as GIF (optional, to view it outside of R)
anim_save("animated_scatter_plot.gif", animation = anim)
library(gganimate)
library(ggplot2)
# Create the scatter plot with animation
p <- ggplot(df, aes(x = zipf_adherence, y = ai_words_rate, color = as.factor(generated))) +
geom_point(alpha = 0.7, size = 3) +
labs(title = 'AI vs. Human Text Analysis: Frame {frame}', x = 'Zipf Adherence', y = 'AI Words Rate') +
theme_minimal() +
transition_states(generated, transition_length = 2, state_length = 1) +
ease_aes('linear')
# Animate the plot and save it
anim <- animate(p, duration = 5, fps = 10, width = 600, height = 400)
# Save the animation as a GIF
gganimate::anim_save("animated_scatter_plot.gif", animation = anim)
library(gganimate)
library(ggplot2)
# Create the scatter plot with animation
p <- ggplot(df, aes(x = zipf_adherence, y = ai_words_rate, color = as.factor(generated))) +
geom_point(alpha = 0.7, size = 3) +
labs(title = 'AI vs. Human Text Analysis: Frame {frame}', x = 'Zipf Adherence', y = 'AI Words Rate') +
theme_minimal() +
transition_states(generated, transition_length = 2, state_length = 1) +
ease_aes('linear')
# Animate the plot
anim <- animate(p, duration = 5, fps = 10, width = 600, height = 400)
# Check if the animation renders in RStudio viewer
anim
install.packages("gifski")
library(gifski)
library(gganimate)
library(ggplot2)
# Create the scatter plot with animation
p <- ggplot(df, aes(x = zipf_adherence, y = ai_words_rate, color = as.factor(generated))) +
geom_point(alpha = 0.7, size = 3) +
labs(title = 'AI vs. Human Text Analysis: Frame {frame}', x = 'Zipf Adherence', y = 'AI Words Rate') +
theme_minimal() +
transition_states(generated, transition_length = 2, state_length = 1) +
ease_aes('linear')
# Animate the plot
anim <- animate(p, duration = 5, fps = 10, width = 600, height = 400)
# Check if the animation renders in RStudio viewer
anim
# Use the generated PNG files and convert them into a GIF
png_files <- list.files(pattern = "gganim_plot.*\\.png")  # List of saved PNG files
# Create GIF from the PNG files
gifski(png_files, gif_file = "animated_scatter_plot.gif", width = 600, height = 400, delay = 1/10)
# Now you have the animated GIF saved as "animated_scatter_plot.gif"
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("caret")
install.packages("randomForest")
data <- read.csv("C:/Users/aaroc/Downloads/AI_Human_features.csv")
# Check the structure and summary of the data
str(data)
summary(data)
# Check the structure and summary of the data
str(data)
print(" ")
summary(data)
# Check the structure and summary of the data
str(data)
print()
# Check the structure and summary of the data
str(data)
summary(data)
# Compute and visualize the correlation matrix for numeric predictors
num_vars <- data %>% select_if(is.numeric)
# Compute and visualize the correlation matrix for numeric predictors
num_vars <- data %>% select_if(is.numeric)
# Compute and visualize the correlation matrix for numeric predictors
correlation_matrix <- cor(data[, -1])
corrplot(correlation_matrix, method = "color")
# Compute and visualize the correlation matrix for numeric predictors
correlation_matrix <- cor(data[, -1])
# Compute and visualize the correlation matrix for numeric predictors
correlation_matrix <- cor(data[, -1])
correlation_matrix
# Example plot: Density plot for lexical_diversity by generated class
ggplot(data, aes(x = lexical_diversity, fill = factor(generated))) +
geom_density(alpha = 0.5) +
labs(title = "Density of Lexical Diversity",
x = "Lexical Diversity",
fill = "Generated\n(0 = Human, 1 = AI)")
# Example plot: Density plot for lexical_diversity by generated class
ggplot2(data, aes(x = lexical_diversity, fill = factor(generated))) +
geom_density(alpha = 0.5) +
labs(title = "Density of Lexical Diversity",
x = "Lexical Diversity",
fill = "Generated\n(0 = Human, 1 = AI)")
# Example plot: Density plot for lexical_diversity by generated class
ggplot(data, aes(x = lexical_diversity, fill = factor(generated))) +
geom_density(alpha = 0.5) +
labs(title = "Density of Lexical Diversity",
x = "Lexical Diversity",
fill = "Generated\n(0 = Human, 1 = AI)")
install.packages("ggplot2")
# Example plot: Density plot for lexical_diversity by generated class
ggplot(data, aes(x = lexical_diversity, fill = factor(generated))) +
geom_density(alpha = 0.5) +
labs(title = "Density of Lexical Diversity",
x = "Lexical Diversity",
fill = "Generated\n(0 = Human, 1 = AI)")
# Split the data into training (70%) and testing (30%) sets
set.seed(123)
trainIndex <- createDataPartition(data$generated, p = 0.7, list = FALSE)
# Split the data into training (70%) and testing (30%) sets
library(caret)
set.seed(123)
trainIndex <- createDataPartition(data$generated, p = 0.7, list = FALSE)
trainData <- data[trainIndex, ]
testData  <- data[-trainIndex, ]
# Feature scaling (center & scale) for predictors only
# We leave the 'generated' column as is.
preProcValues <- preProcess(trainData[, -which(names(trainData) == "generated")],
method = c("center", "scale"))
trainData_scaled <- trainData
trainData_scaled[, -which(names(trainData) == "generated")] <- predict(preProcValues, trainData[, -which(names(trainData) == "generated")])
testData_scaled <- testData
testData_scaled[, -which(names(testData) == "generated")] <- predict(preProcValues, testData[, -which(names(testData) == "generated")])
# Feature scaling (center & scale) for predictors only
# We leave the 'generated' column as is.
preProcValues <- preProcess(trainData[, -which(names(trainData) == "generated")],
method = c("center", "scale"))
trainData_scaled <- trainData
trainData_scaled[, -which(names(trainData) == "generated")] <- predict(preProcValues, trainData[, -which(names(trainData) == "generated")])
testData_scaled <- testData
testData_scaled[, -which(names(testData) == "generated")] <- predict(preProcValues, testData[, -which(names(testData) == "generated")])
# Logistic Regression ---
# Use the scaled data with numeric target (0/1) for logistic regression
log_model <- glm(generated ~ ., data = trainData_scaled, family = binomial)
summary(log_model)
# Predict probabilities on the test set and classify using a 0.5 cutoff
pred_probs_log <- predict(log_model, newdata = testData_scaled, type = "response")
pred_classes_log <- ifelse(pred_probs_log > 0.5, 1, 0)
# Predict probabilities on the test set and classify using a 0.5 cutoff
pred_probs_log <- predict(log_model, newdata = testData_scaled, type = "response")
pred_classes_log <- ifelse(pred_probs_log > 0.5, 1, 0)
cm_log <- confusionMatrix(as.factor(pred_classes_log), as.factor(testData_scaled$generated))
print(cm_log)
# Plot ROC curve and compute AUC for logistic regression
roc_log <- roc(testData_scaled$generated, pred_probs_log)
library(ggplot2)       # For plotting
library(caret)         # For data splitting, evaluation, and tuning
library(randomForest)  # For Random Forest model
install.packages("randomForest")
library(ggplot2)       # For plotting
library(caret)         # For data splitting, evaluation, and tuning
library(randomForest)  # For Random Forest model
library(pROC)          # For ROC and AUC analysis
library(corrplot)      # For correlation plots
install.packages("corrplot")
library(ggplot2)       # For plotting
library(caret)         # For data splitting, evaluation, and tuning
library(randomForest)  # For Random Forest model
library(pROC)          # For ROC and AUC analysis
library(corrplot)      # For correlation plots
library(dplyr)         # For data manipulation
# Plot ROC curve and compute AUC for logistic regression
roc_log <- roc(testData_scaled$generated, pred_probs_log)
plot(roc_log, main = "ROC Curve for Logistic Regression")
auc_log <- auc(roc_log)
print(paste("Logistic Regression AUC:", round(auc_log, 3)))
# For caret’s tuning, convert the target to a factor with descriptive labels.
# Create copies of the scaled data with factor targets.
trainDataFactor <- trainData_scaled
testDataFactor  <- testData_scaled
trainDataFactor$generated <- factor(ifelse(trainDataFactor$generated == 1, "AI", "Human"))
testDataFactor$generated  <- factor(ifelse(testDataFactor$generated == 1, "AI", "Human"))
# For caret’s tuning, convert the target to a factor with descriptive labels.
# Create copies of the scaled data with factor targets.
trainDataFactor <- trainData_scaled
testDataFactor  <- testData_scaled
trainDataFactor$generated <- factor(ifelse(trainDataFactor$generated == 1, "AI", "Human"))
testDataFactor$generated  <- factor(ifelse(testDataFactor$generated == 1, "AI", "Human"))
# Train a basic Random Forest model
rf_model <- randomForest(generated ~ ., data = trainDataFactor, ntree = 100)
data <- na.omit(data)
str(data)
# Train a basic Random Forest model
rf_model <- randomForest(generated ~ ., data = trainDataFactor, ntree = 100)
data <- read.csv("C:/Users/aaroc/Downloads/AI_Human_features.csv")
# Check the structure and summary of the data
str(data)
summary(data)
# Check for missing values
cat("Number of missing values:", sum(is.na(data)), "\n")
if(sum(is.na(data)) > 0) {
# Impute missing values using median imputation
impute_mod <- preProcess(data, method = "medianImpute")
data <- predict(impute_mod, newdata = data)
}
# Check for missing values
cat("Number of missing values:", sum(is.na(data)), "\n")
if(sum(is.na(data)) > 0) {
# Impute missing values using median imputation
impute_mod <- preProcess(data, method = "medianImpute")
data <- predict(impute_mod, newdata = data)
}
cat("Number of missing values after imputation:", sum(is.na(data)), "\n")
# Compute and visualize the correlation matrix for numeric predictors
correlation_matrix <- cor(data[, -1])
correlation_matrix
# Split the data into training (70%) and testing (30%) sets
library(caret)
set.seed(123)
trainIndex <- createDataPartition(data$generated, p = 0.7, list = FALSE)
trainData <- data[trainIndex, ]
testData  <- data[-trainIndex, ]
# Feature scaling (center & scale) for predictors only
# We leave the 'generated' column as is.
preProcValues <- preProcess(trainData[, -which(names(trainData) == "generated")],
method = c("center", "scale"))
trainData_scaled <- trainData
trainData_scaled[, -which(names(trainData) == "generated")] <- predict(preProcValues, trainData[, -which(names(trainData) == "generated")])
testData_scaled <- testData
testData_scaled[, -which(names(testData) == "generated")] <- predict(preProcValues, testData[, -which(names(testData) == "generated")])
# Logistic Regression ---
# Use the scaled data with numeric target (0/1) for logistic regression
log_model <- glm(generated ~ ., data = trainData_scaled, family = binomial)
summary(log_model)
# Predict probabilities on the test set and classify using a 0.5 cutoff
pred_probs_log <- predict(log_model, newdata = testData_scaled, type = "response")
pred_classes_log <- ifelse(pred_probs_log > 0.5, 1, 0)
cm_log <- confusionMatrix(as.factor(pred_classes_log), as.factor(testData_scaled$generated))
print(cm_log)
# Plot ROC curve and compute AUC for logistic regression
roc_log <- roc(testData_scaled$generated, pred_probs_log)
plot(roc_log, main = "ROC Curve for Logistic Regression")
auc_log <- auc(roc_log)
print(paste("Logistic Regression AUC:", round(auc_log, 3)))
# For caret’s tuning, convert the target to a factor with descriptive labels.
# Create copies of the scaled data with factor targets.
trainDataFactor <- trainData_scaled
testDataFactor  <- testData_scaled
trainDataFactor$generated <- factor(ifelse(trainDataFactor$generated == 1, "AI", "Human"))
testDataFactor$generated  <- factor(ifelse(testDataFactor$generated == 1, "AI", "Human"))
# Train a basic Random Forest model
rf_model <- randomForest(generated ~ ., data = trainDataFactor, ntree = 100)
rf_pred <- predict(rf_model, newdata = testDataFactor)
cm_rf <- confusionMatrix(rf_pred, testDataFactor$generated)
print(cm_rf)
# ROC analysis for Random Forest
rf_pred_probs <- predict(rf_model, newdata = testDataFactor, type = "prob")[, "AI"]
roc_rf <- roc(response = testDataFactor$generated, predictor = rf_pred_probs, levels = c("Human", "AI"))
plot(roc_rf, main = "ROC Curve for Random Forest")
auc_rf <- auc(roc_rf)
print(paste("Random Forest AUC:", round(auc_rf, 3)))
# --- 5c. Hyperparameter Tuning for Random Forest using caret ---
control <- trainControl(method = "cv",
number = 5,
search = "grid",
classProbs = TRUE,
summaryFunction = twoClassSummary)
tunegrid <- expand.grid(.mtry = c(2, 3, 4, 5))
set.seed(123)
rf_tuned <- train(generated ~ .,
data = trainDataFactor,
method = "rf",
metric = "ROC",
tuneGrid = tunegrid,
trControl = control)
print(rf_tuned)
# Predict on the test set using the tuned model and evaluate performance
rf_tuned_pred <- predict(rf_tuned, newdata = testDataFactor)
cm_rf_tuned <- confusionMatrix(rf_tuned_pred, testDataFactor$generated)
print(cm_rf_tuned)
# ROC and AUC for the tuned Random Forest
rf_tuned_pred_probs <- predict(rf_tuned, newdata = testDataFactor, type = "prob")[, "AI"]
roc_rf_tuned <- roc(response = testDataFactor$generated,
predictor = rf_tuned_pred_probs,
levels = c("Human", "AI"))
plot(roc_rf_tuned, main = "ROC Curve for Tuned Random Forest")
auc_rf_tuned <- auc(roc_rf_tuned)
print(paste("Tuned Random Forest AUC:", round(auc_rf_tuned, 3)))
saveRDS(rf_tuned, "rf_tuned_model.rds")
saveRDS(preProcValues, "preProcValues.rds")
install.packages("ggplot2")
install.packages("caret")
install.packages("randomForest")
install.packages("pROC")
install.packages("corrplot")
install.packages("dplyr")
library(ggplot2)       # For plotting
library(caret)         # For data splitting, evaluation, and tuning
library(randomForest)  # For Random Forest model
library(pROC)          # For ROC and AUC analysis
library(corrplot)      # For correlation plots
library(dplyr)         # For data manipulation
data <- read.csv("C:/Users/aaroc/Downloads/AI_Human_features.csv")
saveRDS(rf_tuned, "rf_tuned_model.rds")
saveRDS(preProcValues, "preProcValues.rds")
# ---------------------------
# 2. Load Pre-trained Model and Preprocessing Object
# ---------------------------
# Ensure these files are in your working directory or provide the full path.
rf_tuned <- readRDS("rf_tuned_model.rds")
install.packages("hunspell")
install.packages("stringr")
# ---------------------------
# 2. Load Pre-trained Model and Preprocessing Object
# ---------------------------
# Ensure these files are in your working directory or provide the full path.
rf_tuned <- readRDS("./rf_tuned_model.rds")
# ---------------------------
# 2. Load Pre-trained Model and Preprocessing Object
# ---------------------------
# Ensure these files are in your working directory or provide the full path.
getwd()
# ---------------------------
# 2. Load Pre-trained Model and Preprocessing Object
# ---------------------------
# Ensure these files are in your working directory or provide the full path.
setwd("C:/Users/aaroc/MyRepos/AI_Project")
# Get prediction result
result <- predict_text(new_text, preProcValues, rf_tuned)
# 1. Load Required Libraries
library(caret)
library(randomForest)
library(hunspell)
library(stringr)
# 2. Load Pre-trained Model and Preprocessing Object
setwd("C:/Users/aaroc/MyRepos/AI_Project")
rf_tuned <- readRDS("./rf_tuned_model.rds")
preProcValues <- readRDS("./preProcValues.rds")
# 3. Define Feature Calculation Functions
calculate_num_chars <- function(text) {
nchar(text)
}
calculate_burstiness <- function(text) {
sentences <- unlist(strsplit(text, split = "[\\.!?]+"))
sentences <- sentences[nchar(sentences) > 0]
if (length(sentences) < 2) return(0)
sentence_lengths <- sapply(sentences, nchar)
sd(sentence_lengths) / mean(sentence_lengths)
}
calculate_lexical_diversity <- function(text) {
words <- unlist(strsplit(tolower(text), "\\W+"))
words <- words[words != ""]
length(unique(words)) / length(words)
}
calculate_misspelling_rate <- function(text) {
words <- unlist(strsplit(tolower(text), "\\W+"))
words <- words[words != ""]
misspelled <- sum(!hunspell_check(words))
misspelled / length(words)
}
calculate_zipf_adherence <- function(text) {
words <- unlist(strsplit(tolower(text), "\\W+"))
words <- words[words != ""]
freq <- table(words)
freq <- sort(freq, decreasing = TRUE)
ranks <- 1:length(freq)
log_ranks <- log(ranks)
log_freq <- log(as.numeric(freq))
abs(cor(log_ranks, log_freq))
}
calculate_entropy <- function(text) {
words <- unlist(strsplit(tolower(text), "\\W+"))
words <- words[words != ""]
freq <- table(words)
probs <- freq / sum(freq)
-sum(probs * log2(probs))
}
calculate_phrase_repetition <- function(text, n = 2) {
words <- unlist(strsplit(tolower(text), "\\W+"))
words <- words[words != ""]
if (length(words) < n) return(0)
ngrams <- sapply(1:(length(words) - n + 1),
function(i) paste(words[i:(i + n - 1)], collapse = " "))
sum(duplicated(ngrams)) / length(ngrams)
}
calculate_ai_words_rate <- function(text) {
# Define a list of words that may be more common in AI-generated texts.
ai_words <- c("algorithm", "model", "data", "predict", "machine",
"learning", "artificial", "intelligence")
words <- unlist(strsplit(tolower(text), "\\W+"))
words <- words[words != ""]
sum(words %in% ai_words) / length(words)
}
# 4. Function to Compute All Features for a Given Text
compute_features <- function(text) {
data.frame(
num_chars = calculate_num_chars(text),
burstiness = calculate_burstiness(text),
lexical_diversity = calculate_lexical_diversity(text),
misspelling_rate = calculate_misspelling_rate(text),
zipf_adherence = calculate_zipf_adherence(text),
entropy = calculate_entropy(text),
phrase_repetition = calculate_phrase_repetition(text),
ai_words_rate = calculate_ai_words_rate(text)
)
}
# 5. Prediction Function
predict_text <- function(text, preProcValues, model) {
# Compute features from the input text
new_data <- compute_features(text)
# Apply the same scaling/preprocessing as in training
new_data_scaled <- predict(preProcValues, newdata = new_data)
# Predict probabilities using the trained model
prediction_prob <- predict(model, newdata = new_data_scaled, type = "prob")
# Extract probability of the "AI" class (assuming "AI" is the positive class label)
ai_prob <- prediction_prob[, "AI"]
# Decide predicted class based on a 0.5 cutoff
predicted_class <- ifelse(ai_prob > 0.5, "AI", "Human")
list(
prediction = predicted_class,
confidence = ai_prob,
features = new_data  # Optional: Return computed features for review
)
}
# 6. Example: Predict on New Text Input
# Replace the string below with the text you want to evaluate
new_text <- "In recent years, machine learning algorithms have dramatically improved data analysis and predictions in various fields."
# Get prediction result
result <- predict_text(new_text, preProcValues, rf_tuned)
# Print the prediction and its confidence
cat("Predicted Class:", result$prediction, "\n")
cat("Confidence (Probability of AI):", round(result$confidence, 3), "\n")
library(hunspell)
library(stringr)
# Set working directory if needed, or use absolute paths
setwd("C:/Users/aaroc/MyRepos/AI_Project")
shiny::runApp()
